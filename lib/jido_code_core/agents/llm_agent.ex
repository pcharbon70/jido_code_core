defmodule JidoCodeCore.Agents.LLMAgent do
  @moduledoc """
  LLM Agent for handling chat interactions in JidoCodeCore.

  This agent wraps JidoAI's Agent system to provide a coding assistant
  that can be configured with different LLM providers and models at runtime.

  ## Usage

      # Start with default config from JidoCodeCore.Config
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link()

      # Start with custom options
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(
        provider: :openai,
        model: "gpt-4o",
        temperature: 0.5
      )

      # Send a chat message
      {:ok, response} = JidoCodeCore.Agents.LLMAgent.chat(pid, "How do I reverse a list in Elixir?")

  ## Memory Integration

  The agent integrates with the two-tier memory system. Memory is enabled by default
  and can be configured via the `:memory` option:

      # Start with memory disabled
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(
        session_id: "my-session",
        memory: [enabled: false]
      )

      # Start with custom token budget
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(
        session_id: "my-session",
        memory: [token_budget: 16_000]
      )

  Memory options:
  - `:enabled` - Whether to include memory context in prompts (default: `true`)
  - `:token_budget` - Total token budget for context (default: `32_000`)

  When memory is enabled, the agent:
  - Assembles working context and long-term memories before each LLM call
  - Includes formatted memory context in the system prompt
  - Makes memory tools (remember, recall, forget) available for tool use

  ## PubSub Integration

  Responses are broadcast to the session-specific PubSub topic with the event type
  `{:agent_response, response}` for TUI consumption.

  ## Supervision

  This agent can be started under the AgentSupervisor for lifecycle management:

      JidoCodeCore.AgentSupervisor.start_agent(%{
        name: :llm_agent,
        module: JidoCodeCore.Agents.LLMAgent,
        args: []
      })
  """

  use GenServer
  require Logger

  alias Jido.AI.Actions.ReqLlm.ChatCompletion
  alias Jido.AI.Agent, as: AIAgent
  alias Jido.AI.Keyring
  alias Jido.AI.Model
  alias Jido.AI.Model.Registry.Adapter, as: RegistryAdapter
  alias Jido.AI.Prompt
  alias JidoCodeCore.Config
  alias JidoCodeCore.Language
  alias JidoCodeCore.Memory.Actions, as: MemoryActions
  alias JidoCodeCore.Memory.ContextBuilder
  alias JidoCodeCore.Memory.ResponseProcessor
  alias JidoCodeCore.PubSubTopics
  alias JidoCodeCore.Session.ProcessRegistry
  alias JidoCodeCore.Session.State, as: SessionState
  alias JidoCodeCore.Tools.Executor
  alias JidoCodeCore.Tools.Registry, as: ToolRegistry
  alias JidoCodeCore.Tools.Result

  @pubsub JidoCodeCore.PubSub
  @default_timeout 60_000
  @max_message_length 10_000
  @default_token_budget 32_000

  # System prompt should NOT include user input to prevent prompt injection attacks.
  # User messages are passed separately to the AI agent via chat_response/3.
  # The base prompt is extended with language-specific instructions at runtime.
  @base_system_prompt """
  You are JidoCode, an expert coding assistant running in a terminal interface.

  Your capabilities:
  - Answer programming questions across all languages
  - Explain code, algorithms, and concepts
  - Help debug issues and suggest fixes
  - Provide code examples and best practices
  - Assist with architecture and design decisions

  Guidelines:
  - Be concise but thorough - terminal space is limited
  - Use markdown code blocks with language hints
  - When showing code changes, be specific about file locations
  - Ask clarifying questions when requirements are ambiguous
  - Acknowledge limitations when you're uncertain
  """

  # For backwards compatibility and non-session contexts
  @system_prompt @base_system_prompt

  # ============================================================================
  # Client API
  # ============================================================================

  @doc """
  Starts the LLM agent.

  ## Options

  - `:provider` - Override the provider from config (e.g., `:anthropic`, `:openai`)
  - `:model` - Override the model name from config
  - `:temperature` - Override temperature (0.0-1.0)
  - `:max_tokens` - Override max tokens
  - `:name` - GenServer name for registration (optional)
  - `:session_id` - Session ID for PubSub topic isolation (optional, defaults to agent PID string)
  - `:memory` - Memory configuration options (see below)

  ## Memory Options

  The `:memory` option accepts a keyword list:
  - `:enabled` - Whether to include memory context in prompts (default: `true`)
  - `:token_budget` - Total token budget for context assembly (default: `32_000`)

  ## Returns

  - `{:ok, pid}` - Agent started successfully
  - `{:error, reason}` - Failed to start (e.g., invalid config)

  ## Examples

      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link()
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(session_id: "user-123")
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(name: {:via, Registry, {MyRegistry, :llm}})
      {:ok, pid} = JidoCodeCore.Agents.LLMAgent.start_link(session_id: "user-123", memory: [enabled: false])
  """
  @spec start_link(keyword()) :: GenServer.on_start()
  def start_link(opts \\ []) do
    {name, opts} = Keyword.pop(opts, :name)
    server_opts = if name, do: [name: name], else: []
    GenServer.start_link(__MODULE__, opts, server_opts)
  end

  @doc """
  Returns a via tuple for registering an LLMAgent with a session.

  Use this when you want to register an agent in the session registry
  and look it up by session_id later using `Session.Supervisor.get_agent/1`.

  ## Parameters

  - `session_id` - The session's unique identifier (must be a valid UUID)

  ## Returns

  A via tuple suitable for use as the `name` option in `start_link/1`.

  ## Examples

      # Start an agent registered to a session
      {:ok, pid} = LLMAgent.start_link(
        session_id: "550e8400-e29b-41d4-a716-446655440000",
        name: LLMAgent.via("550e8400-e29b-41d4-a716-446655440000")
      )

      # Later, look up by session_id
      {:ok, pid} = Session.Supervisor.get_agent("550e8400-e29b-41d4-a716-446655440000")
  """
  @spec via(String.t()) :: {:via, Registry, {atom(), {atom(), String.t()}}}
  def via(session_id) when is_binary(session_id) do
    ProcessRegistry.via(:agent, session_id)
  end

  @doc """
  Sends a chat message to the agent and returns the response.

  The response is also broadcast to the PubSub `"tui.events"` topic.

  ## Parameters

  - `pid` - The agent process
  - `message` - The user's message string (max #{@max_message_length} characters)
  - `opts` - Options (`:timeout` defaults to 60 seconds)

  ## Returns

  - `{:ok, response}` - Response string from the LLM
  - `{:error, reason}` - Failed to get response

  ## Examples

      {:ok, response} = JidoCodeCore.Agents.LLMAgent.chat(pid, "Explain pattern matching")
  """
  @spec chat(GenServer.server(), String.t(), keyword()) :: {:ok, String.t()} | {:error, term()}
  def chat(pid, message, opts \\ []) when is_binary(message) do
    case validate_message(message) do
      :ok ->
        timeout = Keyword.get(opts, :timeout, @default_timeout)
        GenServer.call(pid, {:chat, message}, timeout + 5_000)

      {:error, _} = error ->
        error
    end
  end

  @doc """
  Sends a chat message to the agent with streaming response.

  Unlike `chat/3`, this function broadcasts response chunks via PubSub as they arrive,
  providing real-time streaming feedback. The function returns immediately after
  starting the stream.

  ## PubSub Events

  The following events are broadcast to the session's PubSub topic:

  - `{:stream_chunk, text}` - Partial response text as it arrives
  - `{:stream_end, full_content}` - Stream completed with full response
  - `{:stream_error, reason}` - Stream failed with error

  ## Parameters

  - `pid` - The agent process
  - `message` - The user's message string (max #{@max_message_length} characters)
  - `opts` - Options (`:timeout` defaults to 60 seconds)

  ## Returns

  - `:ok` - Stream started successfully (responses come via PubSub)
  - `{:error, reason}` - Failed to start stream

  ## Examples

      :ok = JidoCodeCore.Agents.LLMAgent.chat_stream(pid, "Explain pattern matching")
      # Subscribe to PubSub to receive {:stream_chunk, text} events
  """
  @spec chat_stream(GenServer.server(), String.t(), keyword()) :: :ok | {:error, term()}
  def chat_stream(pid, message, opts \\ []) when is_binary(message) do
    case validate_message(message) do
      :ok ->
        timeout = Keyword.get(opts, :timeout, @default_timeout)
        GenServer.cast(pid, {:chat_stream, message, timeout})

      {:error, _} = error ->
        error
    end
  end

  @doc """
  Returns the current model configuration.
  """
  @spec get_config(GenServer.server()) :: map()
  def get_config(pid) do
    GenServer.call(pid, :get_config)
  end

  @doc """
  Returns the session ID and PubSub topic for this agent.

  Use the topic to subscribe to events from this specific agent instance.

  ## Example

      {:ok, session_id, topic} = JidoCodeCore.Agents.LLMAgent.get_session_info(pid)
      Phoenix.PubSub.subscribe(JidoCodeCore.PubSub, topic)
  """
  @spec get_session_info(GenServer.server()) :: {:ok, String.t(), String.t()}
  def get_session_info(pid) do
    GenServer.call(pid, :get_session_info)
  end

  @doc """
  Returns the current status of the agent.

  ## Returns

  `{:ok, status}` where status is a map containing:
  - `:ready` - Boolean indicating if the agent is ready to process messages
  - `:config` - Current LLM configuration
  - `:session_id` - Session identifier
  - `:topic` - PubSub topic for this agent

  ## Example

      {:ok, status} = LLMAgent.get_status(pid)
      if status.ready do
        LLMAgent.chat(pid, "Hello!")
      end
  """
  @spec get_status(GenServer.server()) :: {:ok, map()}
  def get_status(pid) do
    GenServer.call(pid, :get_status)
  end

  @doc """
  Builds a PubSub topic for a given session ID.

  This is useful for subscribing to events before the agent is started.

  ## Example

      topic = JidoCodeCore.Agents.LLMAgent.topic_for_session("user-123")
      Phoenix.PubSub.subscribe(JidoCodeCore.PubSub, topic)
  """
  @spec topic_for_session(String.t()) :: String.t()
  def topic_for_session(session_id) when is_binary(session_id) do
    PubSubTopics.llm_stream(session_id)
  end

  @doc """
  Returns the tool execution context for this agent's session.

  The context is built from the agent's session_id and includes:
  - `session_id` - The session identifier
  - `project_root` - The session's project root path
  - `timeout` - Default tool execution timeout

  This is useful for external code that needs to execute tools
  in the same session context as the agent.

  ## Returns

  - `{:ok, context}` - Context map with session_id, project_root, timeout
  - `{:error, :no_session_id}` - Agent was started without session_id
  - `{:error, :not_found}` - Session not found in registry

  ## Examples

      {:ok, context} = LLMAgent.get_tool_context(pid)
      Tools.Executor.execute(tool_call, context: context)
  """
  @spec get_tool_context(GenServer.server()) :: {:ok, map()} | {:error, term()}
  def get_tool_context(pid) do
    GenServer.call(pid, :get_tool_context)
  end

  @doc """
  Builds a tool execution context from a session_id.

  This is a convenience function that delegates to `Tools.Executor.build_context/1`.
  Use this when you have a session_id but not a running agent.

  ## Parameters

  - `session_id` - The session's unique identifier

  ## Returns

  - `{:ok, context}` - Context map with session_id, project_root, timeout
  - `{:error, :not_found}` - Session not found
  - `{:error, :invalid_session_id}` - Invalid session_id format

  ## Examples

      {:ok, context} = LLMAgent.build_tool_context("550e8400-e29b-41d4-a716-446655440000")
      Tools.Executor.execute(tool_call, context: context)
  """
  @spec build_tool_context(String.t()) :: {:ok, map()} | {:error, term()}
  def build_tool_context(session_id) when is_binary(session_id) do
    Executor.build_context(session_id)
  end

  def build_tool_context(nil), do: {:error, :no_session_id}

  @doc """
  Executes a tool call using the agent's session context.

  The tool call is executed through the session-scoped executor,
  which validates paths and enforces security boundaries.

  ## Parameters

  - `pid` - The agent process
  - `tool_call` - Map with `:id`, `:name`, `:arguments` keys

  ## Returns

  - `{:ok, %Result{}}` - Tool execution result
  - `{:error, :no_session_id}` - Agent was started without proper session_id
  - `{:error, :not_found}` - Session not found in registry
  - `{:error, term()}` - Other execution failures

  ## Examples

      tool_call = %{id: "call_1", name: "read_file", arguments: %{"path" => "/src/main.ex"}}
      {:ok, result} = LLMAgent.execute_tool(pid, tool_call)
      result.status  # => :ok or :error
      result.content # => file contents or error message
  """
  @spec execute_tool(GenServer.server(), map()) ::
          {:ok, Result.t()} | {:error, term()}
  def execute_tool(pid, tool_call) do
    GenServer.call(pid, {:execute_tool, tool_call})
  end

  @doc """
  Executes multiple tool calls using the agent's session context.

  ## Parameters

  - `pid` - The agent process
  - `tool_calls` - List of tool call maps with `:id`, `:name`, `:arguments` keys
  - `opts` - Options

  ## Options

  - `:parallel` - Execute tools in parallel (default: `false`)
  - `:timeout` - Override timeout per tool (default: executor's default)

  ## Returns

  - `{:ok, [%Result{}]}` - List of results in same order as input
  - `{:error, :no_session_id}` - Agent was started without proper session_id
  - `{:error, :not_found}` - Session not found in registry

  ## Examples

      tool_calls = [
        %{id: "call_1", name: "read_file", arguments: %{"path" => "/a.txt"}},
        %{id: "call_2", name: "read_file", arguments: %{"path" => "/b.txt"}}
      ]
      {:ok, results} = LLMAgent.execute_tool_batch(pid, tool_calls, parallel: true)
  """
  @spec execute_tool_batch(GenServer.server(), [map()], keyword()) ::
          {:ok, [Result.t()]} | {:error, term()}
  def execute_tool_batch(pid, tool_calls, opts \\ []) do
    GenServer.call(pid, {:execute_tool_batch, tool_calls, opts})
  end

  @doc """
  Reconfigures the agent with new provider/model settings at runtime.

  This performs hot-swapping of the underlying AI agent without restarting
  the LLMAgent GenServer. The new configuration is validated before being
  applied.

  ## Options

  - `:provider` - New provider atom (e.g., `:anthropic`, `:openai`)
  - `:model` - New model name string
  - `:temperature` - New temperature (0.0-1.0)
  - `:max_tokens` - New max tokens

  ## Validation

  The following validations are performed:
  1. Provider must exist in the ReqLLM registry
  2. Model must exist for the specified provider
  3. API key must be available for the provider

  ## Returns

  - `:ok` - Configuration updated successfully
  - `{:error, reason}` - Validation failed or reconfiguration failed

  ## Examples

      # Switch to OpenAI
      :ok = JidoCodeCore.Agents.LLMAgent.configure(pid, provider: :openai, model: "gpt-4o")

      # Invalid provider
      {:error, "Provider :invalid not found..."} = JidoCodeCore.Agents.LLMAgent.configure(pid, provider: :invalid)
  """
  @spec configure(GenServer.server(), keyword()) :: :ok | {:error, String.t()}
  def configure(pid, opts) when is_list(opts) do
    GenServer.call(pid, {:configure, opts})
  end

  @doc """
  Lists available LLM providers from the ReqLLM registry.

  ## Returns

  - `{:ok, providers}` - List of provider atoms
  - `{:error, reason}` - Registry unavailable

  ## Examples

      {:ok, providers} = JidoCodeCore.Agents.LLMAgent.list_providers()
      # => {:ok, [:anthropic, :openai, :google, ...]}
  """
  @spec list_providers() :: {:ok, [atom()]} | {:error, term()}
  def list_providers do
    # TODO: Replace with RegistryAdapter.list_providers() when available in jido_ai
    # For now, return a static list of known providers
    {:ok, [:anthropic, :openai, :google, :groq, :openrouter]}
  end

  @doc """
  Lists available models for a provider.

  ## Returns

  - `{:ok, models}` - List of model name strings
  - `{:error, reason}` - Provider not found or registry unavailable

  ## Examples

      {:ok, models} = JidoCodeCore.Agents.LLMAgent.list_models(:anthropic)
      # => {:ok, ["claude-3-5-sonnet-20241022", "claude-3-opus-20240229", ...]}
  """
  @spec list_models(atom()) :: {:ok, [String.t()]} | {:error, term()}
  def list_models(provider) when is_atom(provider) do
    case RegistryAdapter.list_models(provider) do
      {:ok, models} -> {:ok, extract_model_names(models)}
      {:error, _} = error -> error
    end
  end

  defp extract_model_names(models) do
    models
    |> Enum.map(&get_model_name/1)
    |> Enum.reject(&is_nil/1)
  end

  defp get_model_name(%{model: name}) when is_binary(name), do: name
  defp get_model_name(_), do: nil

  @doc """
  Returns the list of available tools for the agent.

  When memory is enabled, this includes both the base tools from the
  Tools.Registry and the memory tools (remember, recall, forget).
  When memory is disabled, only the base tools are returned.

  ## Returns

  - `{:ok, tools}` - List of tool definitions in LLM format

  ## Examples

      {:ok, tools} = JidoCodeCore.Agents.LLMAgent.get_available_tools(pid)

  """
  @spec get_available_tools(GenServer.server()) :: {:ok, [map()]}
  def get_available_tools(pid) do
    GenServer.call(pid, :get_available_tools)
  end

  @doc """
  Checks if a tool name is a memory tool.

  Memory tools are: remember, recall, forget

  ## Examples

      JidoCodeCore.Agents.LLMAgent.memory_tool?("remember")
      # => true

      JidoCodeCore.Agents.LLMAgent.memory_tool?("read_file")
      # => false

  """
  @spec memory_tool?(String.t()) :: boolean()
  def memory_tool?(name) when is_binary(name) do
    MemoryActions.memory_action?(name)
  end

  def memory_tool?(_), do: false

  # ============================================================================
  # GenServer Callbacks
  # ============================================================================

  @impl true
  def init(opts) do
    # Trap exits so we can handle AI agent crashes gracefully
    Process.flag(:trap_exit, true)

    # Extract session_id and memory options before building config
    {session_id, opts} = Keyword.pop(opts, :session_id)
    {memory_opts, config_opts} = Keyword.pop(opts, :memory, [])

    # Validate and normalize token_budget
    token_budget = validate_token_budget(Keyword.get(memory_opts, :token_budget))

    case build_config(config_opts) do
      {:ok, config} ->
        case start_ai_agent(config) do
          {:ok, ai_pid} ->
            # Use provided session_id or generate one based on agent PID
            actual_session_id = session_id || inspect(self())

            state = %{
              ai_pid: ai_pid,
              config: config,
              session_id: actual_session_id,
              topic: build_topic(actual_session_id),
              is_processing: false,
              # Memory integration
              memory_enabled: Keyword.get(memory_opts, :enabled, true),
              token_budget: token_budget
            }

            {:ok, state}

          {:error, reason} ->
            {:stop, reason}
        end

      {:error, reason} ->
        {:stop, reason}
    end
  end

  @impl true
  def handle_info({:EXIT, pid, reason}, %{ai_pid: ai_pid} = state) when pid == ai_pid do
    Logger.warning("AI agent exited: #{inspect(reason)}")
    # Attempt to restart the AI agent
    case start_ai_agent(state.config) do
      {:ok, new_ai_pid} ->
        {:noreply, %{state | ai_pid: new_ai_pid}}

      {:error, _} ->
        {:stop, {:ai_agent_died, reason}, state}
    end
  end

  @impl true
  def handle_info({:EXIT, _pid, _reason}, state) do
    # Ignore other exits
    {:noreply, state}
  end

  @impl true
  def handle_info(:stream_complete, state) do
    # Reset processing state when stream completes (success or failure)
    {:noreply, %{state | is_processing: false}}
  end

  @impl true
  def handle_call({:chat, message}, from, state) do
    # ARCH-1 Fix: Use Task.Supervisor for monitored async tasks
    # This prevents silent failures and hanging callers
    ai_pid = state.ai_pid
    topic = state.topic

    Task.Supervisor.start_child(JidoCodeCore.TaskSupervisor, fn ->
      result = do_chat(ai_pid, message)

      # Broadcast to session-specific PubSub topic for TUI
      case result do
        {:ok, response} ->
          broadcast_response(topic, response)

        _ ->
          :ok
      end

      GenServer.reply(from, result)
    end)

    {:noreply, state}
  end

  @impl true
  def handle_call(:get_config, _from, state) do
    {:reply, state.config, state}
  end

  @impl true
  def handle_call(:get_session_info, _from, state) do
    {:reply, {:ok, state.session_id, state.topic}, state}
  end

  @impl true
  def handle_call(:get_status, _from, state) do
    # ready is false when processing or when agent is not alive
    agent_alive = is_pid(state.ai_pid) and Process.alive?(state.ai_pid)
    ready = agent_alive and not state.is_processing

    status = %{
      ready: ready,
      config: state.config,
      session_id: state.session_id,
      topic: state.topic
    }

    {:reply, {:ok, status}, state}
  end

  @impl true
  def handle_call(:get_tool_context, _from, state) do
    result = do_build_tool_context(state.session_id)
    {:reply, result, state}
  end

  @impl true
  def handle_call({:execute_tool, tool_call}, _from, state) do
    result = do_execute_tool(tool_call, state)
    {:reply, result, state}
  end

  @impl true
  def handle_call({:execute_tool_batch, tool_calls, opts}, _from, state) do
    result = do_execute_tool_batch(tool_calls, opts, state)
    {:reply, result, state}
  end

  @impl true
  def handle_call(:get_available_tools, _from, state) do
    tools = do_get_available_tools(state)
    {:reply, {:ok, tools}, state}
  end

  @impl true
  def handle_call({:configure, opts}, _from, state) do
    # Build new config, merging with current config
    new_config = %{
      provider: Keyword.get(opts, :provider, state.config.provider),
      model: Keyword.get(opts, :model, state.config.model),
      temperature: Keyword.get(opts, :temperature, state.config.temperature),
      max_tokens: Keyword.get(opts, :max_tokens, state.config.max_tokens)
    }

    # Skip validation and restart if config unchanged
    if new_config == state.config do
      {:reply, :ok, state}
    else
      apply_config_change(new_config, state)
    end
  end

  @impl true
  def handle_cast({:chat_stream, message, timeout}, state) do
    topic = state.topic
    config = state.config
    session_id = state.session_id
    agent_pid = self()

    # Extract memory options for context assembly
    memory_opts = extract_memory_opts(state)

    # ARCH-1 Fix: Use Task.Supervisor for monitored async streaming
    Task.Supervisor.start_child(JidoCodeCore.TaskSupervisor, fn ->
      # Trap exits to prevent ReqLLM's internal cleanup tasks from crashing us
      Process.flag(:trap_exit, true)

      try do
        do_chat_stream_with_timeout(config, message, topic, timeout, session_id, memory_opts)
        # Notify agent that streaming is complete
        send(agent_pid, :stream_complete)
      catch
        :exit, {:timeout, _} ->
          Logger.warning("Stream timed out after #{timeout}ms")
          broadcast_stream_error(topic, :timeout)
          send(agent_pid, :stream_complete)

        kind, reason ->
          Logger.error("Stream failed: #{kind} - #{inspect(reason)}")
          broadcast_stream_error(topic, {kind, reason})
          send(agent_pid, :stream_complete)
      end

      # Drain any EXIT messages from ReqLLM cleanup tasks
      receive do
        {:EXIT, _pid, _reason} -> :ok
      after
        100 -> :ok
      end
    end)

    {:noreply, %{state | is_processing: true}}
  end

  @impl true
  def terminate(_reason, state) do
    # Stop the AI agent if running - catch any errors silently
    # Use consistent dot access since ai_pid is always present in state
    ai_pid = state.ai_pid

    if ai_pid && Process.alive?(ai_pid) do
      try do
        GenServer.stop(ai_pid, :normal, 1000)
      catch
        :exit, _ -> :ok
      end
    end

    :ok
  end

  # ============================================================================
  # Private Functions
  # ============================================================================

  defp build_config(opts) do
    config_result =
      case Config.get_llm_config() do
        {:ok, base_config} ->
          config = %{
            provider: Keyword.get(opts, :provider, base_config.provider),
            model: Keyword.get(opts, :model, base_config.model),
            temperature: Keyword.get(opts, :temperature, base_config.temperature),
            max_tokens: Keyword.get(opts, :max_tokens, base_config.max_tokens)
          }

          {:ok, config}

        {:error, _reason} = error ->
          # Allow starting with explicit opts even without base config
          if Keyword.has_key?(opts, :provider) and Keyword.has_key?(opts, :model) do
            config = %{
              provider: Keyword.fetch!(opts, :provider),
              model: Keyword.fetch!(opts, :model),
              temperature: Keyword.get(opts, :temperature, 0.7),
              max_tokens: Keyword.get(opts, :max_tokens, 4096)
            }

            {:ok, config}
          else
            error
          end
      end

    # Validate config before returning to ensure agents cannot start in invalid state
    case config_result do
      {:ok, config} ->
        case validate_config(config) do
          :ok -> {:ok, config}
          {:error, reason} -> {:error, reason}
        end

      error ->
        error
    end
  end

  defp start_ai_agent(config) do
    model_tuple =
      {config.provider,
       [
         model: config.model,
         temperature: config.temperature,
         max_tokens: config.max_tokens
       ]}

    case Model.from(model_tuple) do
      {:ok, model} ->
        AIAgent.start_link(
          ai: [
            model: model,
            prompt: @system_prompt
          ]
        )

      {:error, reason} ->
        {:error, {:model_error, reason}}
    end
  end

  defp do_chat(ai_pid, message) do
    case AIAgent.chat_response(ai_pid, message, timeout: @default_timeout) do
      {:ok, %{response: response}} when is_binary(response) ->
        {:ok, response}

      {:ok, response} when is_binary(response) ->
        {:ok, response}

      {:ok, %{} = result} ->
        # Handle other response formats
        response = Map.get(result, :response) || Map.get(result, :content) || inspect(result)
        {:ok, response}

      {:error, reason} ->
        Logger.error("LLM chat error: #{inspect(reason)}")
        {:error, reason}
    end
  end

  defp build_topic(session_id) do
    PubSubTopics.llm_stream(session_id)
  end

  # Build tool execution context from session_id
  # Returns {:ok, context} or {:error, reason}
  defp do_build_tool_context(nil), do: {:error, :no_session_id}

  defp do_build_tool_context(session_id) when is_binary(session_id) do
    # Reuse is_valid_session_id?/1 to avoid duplicating PID string check
    if is_valid_session_id?(session_id) do
      Executor.build_context(session_id)
    else
      {:error, :no_session_id}
    end
  end

  # Execute a single tool using the session context
  defp do_execute_tool(tool_call, %{session_id: session_id} = _state) do
    with {:ok, context} <- do_build_tool_context(session_id) do
      Executor.execute(tool_call, context: context)
    end
  end

  # Execute multiple tools using the session context
  defp do_execute_tool_batch(tool_calls, opts, %{session_id: session_id} = _state) do
    with {:ok, context} <- do_build_tool_context(session_id) do
      # Merge context into opts for execute_batch
      batch_opts = Keyword.put(opts, :context, context)
      Executor.execute_batch(tool_calls, batch_opts)
    end
  end

  # Returns the list of available tools based on memory configuration.
  # When memory is enabled, includes memory tools (remember, recall, forget).
  @spec do_get_available_tools(map()) :: list(map())
  defp do_get_available_tools(%{memory_enabled: true}) do
    ToolRegistry.to_llm_format() ++ MemoryActions.to_tool_definitions()
  end

  # Default to base tools if memory_enabled is false or not in state (backwards compat)
  defp do_get_available_tools(_state) do
    ToolRegistry.to_llm_format()
  end

  defp broadcast_response(topic, response) do
    Phoenix.PubSub.broadcast(@pubsub, topic, {:agent_response, response})
  end

  # CQ-2 Fix: Standardize config_changed to 2-tuple format
  # Previously used 3-tuple {:config_changed, old_config, new_config}
  # Now uses 2-tuple {:config_changed, new_config} for consistency with commands.ex
  defp broadcast_config_change(topic, _old_config, new_config) do
    Phoenix.PubSub.broadcast(@pubsub, topic, {:config_changed, new_config})
  end

  defp broadcast_stream_chunk(topic, chunk, session_id) do
    # Update Session.State with chunk (skip if session_id is PID string)
    update_session_streaming(session_id, chunk)
    # Also broadcast for TUI (include session_id for routing)
    Phoenix.PubSub.broadcast(@pubsub, topic, {:stream_chunk, session_id, chunk})
  end

  defp broadcast_stream_end(topic, full_content, session_id, metadata) do
    # Finalize message in Session.State (skip if session_id is PID string)
    end_session_streaming(session_id)
    # Also broadcast for TUI (include session_id, content, and metadata for routing)
    Phoenix.PubSub.broadcast(@pubsub, topic, {:stream_end, session_id, full_content, metadata})

    # Process response for context extraction (async to not block stream completion)
    process_response_async(full_content, session_id)
  end

  # Asynchronously process LLM response for context extraction
  # Runs in a separate task to avoid blocking stream completion
  @spec process_response_async(String.t(), String.t()) :: :ok
  defp process_response_async(full_content, session_id) when is_binary(session_id) do
    if is_valid_session_id?(session_id) do
      Task.start(fn ->
        {:ok, extractions} = ResponseProcessor.process_response(full_content, session_id)

        if map_size(extractions) > 0 do
          Logger.debug(
            "LLMAgent: Extracted #{map_size(extractions)} context items from response: #{inspect(Map.keys(extractions))}"
          )
        end
      end)
    end

    :ok
  end

  defp process_response_async(_content, _session_id), do: :ok

  defp broadcast_stream_error(topic, reason) do
    Phoenix.PubSub.broadcast(@pubsub, topic, {:stream_error, reason})
  end

  # Wrapper that enforces timeout on stream operations
  defp do_chat_stream_with_timeout(config, message, topic, timeout, session_id, memory_opts) do
    # Use a Task to enforce timeout on the entire streaming operation
    task =
      Task.async(fn ->
        do_chat_stream(config, message, topic, session_id, memory_opts)
      end)

    case Task.yield(task, timeout) || Task.shutdown(task, :brutal_kill) do
      {:ok, _result} ->
        :ok

      nil ->
        # Task was killed due to timeout
        Logger.warning("Streaming operation timed out after #{timeout}ms")
        broadcast_stream_error(topic, :timeout)
    end
  end

  defp do_chat_stream(config, message, topic, session_id, memory_opts) do
    # Build model from config
    model_tuple =
      {config.provider,
       [
         model: config.model,
         temperature: config.temperature,
         max_tokens: config.max_tokens
       ]}

    case Model.from(model_tuple) do
      {:ok, model} ->
        execute_stream(model, message, topic, session_id, memory_opts)

      {:error, reason} ->
        Logger.error("Failed to create model for streaming: #{inspect(reason)}")
        broadcast_stream_error(topic, {:model_error, reason})
    end
  end

  defp execute_stream(model, message, topic, session_id, memory_opts) do
    # Build memory context if enabled and session is valid
    memory_context = build_memory_context(session_id, message, memory_opts)

    # Build dynamic system prompt with language-specific instructions and memory context
    system_prompt = build_system_prompt(session_id, memory_context)

    # Build prompt with system message and user message
    prompt =
      Prompt.new(%{
        messages: [
          %{role: :system, content: system_prompt, engine: :none},
          %{role: :user, content: message, engine: :none}
        ]
      })

    # Generate a unique message ID for this streaming response
    message_id = "msg_" <> Base.encode16(:crypto.strong_rand_bytes(8), case: :lower)

    # Start streaming in Session.State (skip if session_id is PID string)
    start_session_streaming(session_id, message_id)

    # Execute streaming chat completion
    case ChatCompletion.run(
           %{
             model: model,
             prompt: prompt,
             stream: true
           },
           %{}
         ) do
      {:ok, stream} ->
        process_stream(stream, topic, session_id)

      {:error, reason} ->
        Logger.error("Failed to start stream: #{inspect(reason)}")
        broadcast_stream_error(topic, reason)
    end
  end

  defp process_stream(stream_response, topic, session_id) do
    # Extract inner stream from StreamResponse
    # Note: ReqLLM.StreamResponse now uses direct metadata access instead of metadata_handle
    actual_stream =
      case stream_response do
        %ReqLLM.StreamResponse{stream: inner} when inner != nil ->
          inner

        %ReqLLM.StreamResponse{} ->
          Logger.warning("LLMAgent: StreamResponse has nil stream field")
          []

        other ->
          # Not a StreamResponse, just a raw stream
          other
      end

    # Accumulate full content while streaming chunks (catch handles ReqLLM process cleanup race conditions)
    full_content =
      try do
        Enum.reduce_while(actual_stream, "", fn chunk, acc ->
          case extract_chunk_content(chunk) do
            {:ok, content} ->
              # Only broadcast non-empty content
              if content != "" do
                broadcast_stream_chunk(topic, content, session_id)
              end

              {:cont, acc <> content}

            {:finish, content} ->
              if content != "" do
                broadcast_stream_chunk(topic, content, session_id)
              end

              {:halt, acc <> content}
          end
        end)
      rescue
        e in Protocol.UndefinedError ->
          Logger.error("LLMAgent: Protocol error during enumeration: #{inspect(e)}")
          broadcast_stream_error(topic, e)
          ""

        e ->
          Logger.error("LLMAgent: Error during enumeration: #{inspect(e)}")
          broadcast_stream_error(topic, e)
          ""
      catch
        :exit, {:noproc, _} ->
          # StreamServer died - this is expected at end of stream due to ReqLLM cleanup
          Logger.debug("LLMAgent: Stream server terminated (normal cleanup)")
          ""

        :exit, reason ->
          Logger.warning("LLMAgent: Stream exited: #{inspect(reason)}")
          ""
      end

    # Await metadata from the stream (usage info, finish_reason, etc.)
    # This keeps the StreamServer alive until metadata is collected
    metadata = await_stream_metadata(stream_response)

    # Log usage information if available
    if metadata[:usage] do
      Logger.info("LLMAgent: Token usage - #{inspect(metadata[:usage])}")
    end

    # Broadcast stream completion and finalize in Session.State
    broadcast_stream_end(topic, full_content, session_id, metadata)
  rescue
    error ->
      Logger.error("Stream processing error: #{inspect(error)}")
      broadcast_stream_error(topic, error)
  end

  # Await metadata from the stream's metadata
  # Returns empty map if not a StreamResponse or metadata extraction fails
  defp await_stream_metadata(%ReqLLM.StreamResponse{} = stream_response) do
    try do
      %{
        usage: ReqLLM.StreamResponse.usage(stream_response),
        finish_reason: ReqLLM.StreamResponse.finish_reason(stream_response)
      }
    rescue
      error ->
        Logger.warning("LLMAgent: Failed to extract stream metadata: #{inspect(error)}")
        %{}
    end
  end

  defp await_stream_metadata(_), do: %{}

  # ReqLLM.StreamChunk format - content type with text field
  defp extract_chunk_content(%ReqLLM.StreamChunk{type: :content, text: text}) do
    {:ok, text || ""}
  end

  # ReqLLM.StreamChunk format - meta type signals end of stream
  defp extract_chunk_content(%ReqLLM.StreamChunk{type: :meta, metadata: metadata}) do
    if Map.get(metadata, :finish_reason) do
      {:finish, ""}
    else
      {:ok, ""}
    end
  end

  # ReqLLM.StreamChunk format - thinking/tool_call types (skip for now)
  defp extract_chunk_content(%ReqLLM.StreamChunk{type: _type}) do
    {:ok, ""}
  end

  # Legacy format - content with finish_reason
  defp extract_chunk_content(%{content: content, finish_reason: nil}) do
    {:ok, content || ""}
  end

  defp extract_chunk_content(%{content: content, finish_reason: _reason}) do
    {:finish, content || ""}
  end

  # Legacy format - delta content
  defp extract_chunk_content(%{delta: %{content: content}} = chunk) do
    finish_reason = Map.get(chunk, :finish_reason)

    if finish_reason do
      {:finish, content || ""}
    else
      {:ok, content || ""}
    end
  end

  defp extract_chunk_content(chunk) when is_binary(chunk) do
    {:ok, chunk}
  end

  defp extract_chunk_content(chunk) do
    # Unknown chunk format - try to extract content or text
    content = Map.get(chunk, :content) || Map.get(chunk, :text) || Map.get(chunk, "content") || ""
    {:ok, content}
  end

  defp stop_ai_agent(ai_pid) when is_pid(ai_pid) do
    if Process.alive?(ai_pid) do
      try do
        GenServer.stop(ai_pid, :normal, 1000)
      catch
        :exit, _ -> :ok
      end
    end
  end

  defp stop_ai_agent(_), do: :ok

  defp apply_config_change(new_config, state) do
    case validate_config(new_config) do
      :ok ->
        stop_ai_agent(state.ai_pid)
        do_apply_config(new_config, state)

      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  defp do_apply_config(new_config, state) do
    case start_ai_agent(new_config) do
      {:ok, new_ai_pid} ->
        old_config = state.config
        new_state = %{state | ai_pid: new_ai_pid, config: new_config}
        broadcast_config_change(state.topic, old_config, new_config)
        {:reply, :ok, new_state}

      {:error, reason} ->
        restore_old_config(reason, state)
    end
  end

  defp restore_old_config(reason, state) do
    case start_ai_agent(state.config) do
      {:ok, restored_pid} ->
        {:reply, {:error, "Failed to apply new config: #{inspect(reason)}"},
         %{state | ai_pid: restored_pid}}

      {:error, _} ->
        {:stop, {:error, :cannot_restore_agent}, {:error, reason}, state}
    end
  end

  # ============================================================================
  # Session.State Streaming Helpers
  # ============================================================================

  # Start streaming in Session.State (skip if session_id is PID string)
  defp start_session_streaming(session_id, message_id) when is_binary(session_id) do
    if is_valid_session_id?(session_id) do
      SessionState.start_streaming(session_id, message_id)
    else
      :ok
    end
  end

  # Update streaming content in Session.State (skip if session_id is PID string)
  defp update_session_streaming(session_id, chunk) when is_binary(session_id) do
    if is_valid_session_id?(session_id) do
      SessionState.update_streaming(session_id, chunk)
    else
      :ok
    end
  end

  # End streaming in Session.State (skip if session_id is PID string)
  defp end_session_streaming(session_id) when is_binary(session_id) do
    if is_valid_session_id?(session_id) do
      SessionState.end_streaming(session_id)
    else
      :ok
    end
  end

  # Check if session_id is a valid session ID (not a PID string)
  @spec is_valid_session_id?(String.t()) :: boolean()
  defp is_valid_session_id?(session_id) when is_binary(session_id) do
    not String.starts_with?(session_id, "#PID<")
  end

  # Extract memory options from agent state
  # Provides consistent access with defaults for backwards compatibility
  @spec extract_memory_opts(map()) :: map()
  defp extract_memory_opts(state) do
    %{
      memory_enabled: Map.get(state, :memory_enabled, true),
      token_budget: Map.get(state, :token_budget, @default_token_budget)
    }
  end

  # ============================================================================
  # System Prompt Building
  # ============================================================================

  # Build the system prompt with optional language-specific instructions and memory context
  @spec build_system_prompt(String.t(), ContextBuilder.context() | nil) :: String.t()
  defp build_system_prompt(session_id, memory_context) when is_binary(session_id) do
    base_prompt =
      if is_valid_session_id?(session_id) do
        case SessionState.get_state(session_id) do
          {:ok, %{session: session}} when not is_nil(session.language) ->
            add_language_instruction(@base_system_prompt, session.language)

          _ ->
            @base_system_prompt
        end
      else
        @base_system_prompt
      end

    add_memory_context(base_prompt, memory_context)
  end

  defp build_system_prompt(_, memory_context) do
    add_memory_context(@base_system_prompt, memory_context)
  end

  # Add language-specific instruction to the system prompt
  defp add_language_instruction(base_prompt, language) do
    lang_name = Language.display_name(language)

    language_instruction = """

    Language Context:
    This project uses #{lang_name}. When providing code snippets, write them in #{lang_name} unless a different language is specifically requested.
    """

    base_prompt <> language_instruction
  end

  # Add memory context to the system prompt if available
  @spec add_memory_context(String.t(), ContextBuilder.context() | nil) :: String.t()
  defp add_memory_context(prompt, nil), do: prompt

  defp add_memory_context(prompt, memory_context) do
    memory_section = ContextBuilder.format_for_prompt(memory_context)

    if memory_section != "" do
      prompt <> "\n\n" <> memory_section
    else
      prompt
    end
  end

  # Build memory context for the current message
  # Uses the token_budget from agent state, falling back to ContextBuilder defaults
  @spec build_memory_context(String.t(), String.t(), map()) :: ContextBuilder.context() | nil
  defp build_memory_context(session_id, message, memory_opts) do
    if memory_opts.memory_enabled and is_valid_session_id?(session_id) do
      # Build custom budget using agent's token_budget for total, with proportional splits
      budget = build_token_budget(memory_opts.token_budget)

      case ContextBuilder.build(session_id,
             token_budget: budget,
             query_hint: message
           ) do
        {:ok, context} ->
          Logger.debug("LLMAgent: Built memory context with #{context.token_counts.total} tokens")
          context

        {:error, reason} ->
          Logger.debug("LLMAgent: Failed to build memory context: #{inspect(reason)}")
          nil
      end
    else
      nil
    end
  end

  # Build a proportional token budget based on the total budget
  # Delegates to ContextBuilder.allocate_budget/1 for consistent allocation
  @spec build_token_budget(pos_integer()) :: ContextBuilder.token_budget()
  defp build_token_budget(total), do: ContextBuilder.allocate_budget(total)

  # ============================================================================
  # Validation Functions
  # ============================================================================

  # Validate and normalize token_budget to a positive integer within bounds
  # Returns the validated budget or the default if invalid
  @min_token_budget 1_000
  @max_token_budget 200_000

  @spec validate_token_budget(term()) :: pos_integer()
  defp validate_token_budget(nil), do: @default_token_budget

  defp validate_token_budget(budget)
       when is_integer(budget) and budget >= @min_token_budget and budget <= @max_token_budget do
    budget
  end

  defp validate_token_budget(budget)
       when is_integer(budget) and budget > 0 and budget < @min_token_budget do
    Logger.warning("Token budget #{budget} is below minimum #{@min_token_budget}, using minimum")
    @min_token_budget
  end

  defp validate_token_budget(budget) when is_integer(budget) and budget > @max_token_budget do
    Logger.warning("Token budget #{budget} exceeds maximum #{@max_token_budget}, using maximum")
    @max_token_budget
  end

  defp validate_token_budget(budget) do
    Logger.warning(
      "Invalid token_budget #{inspect(budget)}, using default #{@default_token_budget}"
    )

    @default_token_budget
  end

  defp validate_message(message) when byte_size(message) > @max_message_length do
    {:error,
     {:message_too_long,
      "Message exceeds maximum length of #{@max_message_length} characters (received #{byte_size(message)})"}}
  end

  defp validate_message(message) when byte_size(message) == 0 do
    {:error, {:empty_message, "Message cannot be empty"}}
  end

  defp validate_message(_message), do: :ok

  defp validate_config(config) do
    with :ok <- validate_provider(config.provider),
         :ok <- validate_model(config.provider, config.model) do
      validate_api_key(config.provider)
    end
  end

  defp validate_provider(provider) do
    case list_providers() do
      {:ok, providers} ->
        if provider in providers do
          :ok
        else
          available =
            providers
            |> Enum.take(10)
            |> Enum.map_join(", ", &Atom.to_string/1)

          {:error,
           "Provider '#{provider}' not found. Available providers include: #{available}... (#{length(providers)} total)"}
        end

      {:error, reason} ->
        {:error, "Failed to validate provider: #{inspect(reason)}"}
    end
  end

  defp validate_model(_provider, _model) do
    # TODO: Implement model validation when RegistryAdapter.model_exists? is available in jido_ai
    # For now, always allow any model
    :ok
  end

  defp validate_api_key(provider) do
    api_key_name = :"#{provider}_api_key"
    env_key = api_key_name |> Atom.to_string() |> String.upcase()

    # Check environment variable directly
    case System.get_env(env_key) do
      nil ->
        # Try Keyring as fallback
        try_keyring_validation(provider, api_key_name, env_key)

      "" ->
        {:error,
         "API key for provider '#{provider}' is empty. Set #{env_key} environment variable."}

      _key ->
        :ok
    end
  end

  defp try_keyring_validation(provider, api_key_name, env_key) do
    case Keyring.get(api_key_name, nil) do
      nil ->
        {:error,
         "No API key found for provider '#{provider}'. Set #{env_key} environment variable."}

      "" ->
        {:error,
         "API key for provider '#{provider}' is empty. Set #{env_key} environment variable."}

      _key ->
        :ok
    end
  rescue
    _ ->
      {:error,
       "No API key found for provider '#{provider}'. Set #{env_key} environment variable."}
  catch
    :exit, _ ->
      {:error,
       "No API key found for provider '#{provider}'. Set #{env_key} environment variable."}
  end
end
